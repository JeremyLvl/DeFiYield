{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QUERY DATA\n",
    "\n",
    "endpoint = f\"https://public-api.defiyield.app/graphql/\"\n",
    "key = {\"X-Api-Key\" : \"bad7014e-3fd4-473a-b981-1a5c2dd2d72e\"}\n",
    "\n",
    "def subQuery(pageNumber : int):\n",
    "    query = \"\"\"\n",
    "    query {\n",
    "        rekts (pageNumber: \"\"\"+str(pageNumber)+\"\"\") {\n",
    "            id\n",
    "            projectName\n",
    "            date\n",
    "            fundsLost\n",
    "            fundsReturned\n",
    "            category\n",
    "            issueType\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "    result = requests.post(endpoint, json={\"query\": query}, headers=key)\n",
    "    df=pd.DataFrame.from_dict(result.json()['data']['rekts'])\n",
    "    return df\n",
    "\n",
    "\n",
    "#construct full dataset and store\n",
    "subDf = []\n",
    "for i in range(1,64):\n",
    "    subDf.append(subQuery(i))\n",
    "df = pd.concat(subDf)\n",
    "df['date']=pd.to_datetime(df.date, format='%Y-%m-%d')\n",
    "df['fundsLost'] = pd.to_numeric(df.fundsLost)\n",
    "df['fundsReturned'] = pd.to_numeric(df.fundsReturned)\n",
    "df['count'] = 1\n",
    "df.to_pickle('fullData_01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD LOCALLY\n",
    "\n",
    "df = pd.read_pickle('fullData_01')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTIONS FOR NICER VISUALISATIONS - ignore\n",
    "\n",
    "#convert to postfix\n",
    "def postfix(textStr, usd):\n",
    "    usdStr = ''\n",
    "    if usd:\n",
    "        usdStr = '$'\n",
    "\n",
    "    text_transform = (\n",
    "            lambda x:\n",
    "            f\"{usdStr}{round(x//1000000000)}B\"\n",
    "                if x / 1000000000 >= 1\n",
    "            else f\"{usdStr}{round(x//1000000)}M\"\n",
    "                if x / 1000000 >= 1\n",
    "            else f\"{usdStr}{round(x//1000)}K\"\n",
    "                if x / 10000 >= 1\n",
    "            else f\"{usdStr}{round(x)}\"\n",
    "                if x >= 1\n",
    "            else f\"{usdStr}{round(x, 2)}\"\n",
    "        )\n",
    "    \n",
    "    return text_transform(float(textStr))\n",
    "\n",
    "#annotate plot with postfix\n",
    "def annot(plot, textStr, usd):\n",
    "    if textStr == 'labels':\n",
    "        for t in plot.texts:\n",
    "            current_text = t.get_text()\n",
    "            t.set_text(postfix(current_text, usd))\n",
    "\n",
    "    if textStr == 'yticks':\n",
    "        current_text = plot.get_yticks()\n",
    "        new_text = [postfix(t, usd) for t in current_text]\n",
    "        plot.set_yticklabels(new_text)\n",
    "\n",
    "#order pivot table for heatmap presentation\n",
    "def orderPivot(df):\n",
    "    s = df.sum()\n",
    "    ss = s.sort_values(ascending=False)\n",
    "    df1 = df[ss.index]\n",
    "    s2 = df.sum(axis=1)\n",
    "    ss2 = s2.sort_values()\n",
    "    df1 = df1.reindex(ss2.index)\n",
    "    return df1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we recreate the cumulative sum of lost and recovered funds from the Rekt homepage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sum of lost and recovered funds\n",
    "sumDf = df.copy()\n",
    "sumDf.date = sumDf.date.dt.strftime('%Y-%m')\n",
    "\n",
    "sumTable = pd.pivot_table(sumDf, values=['fundsLost','fundsReturned'], index='date', aggfunc=np.sum)\n",
    "sumTable['lostSum'] = sumTable.fundsLost.cumsum()\n",
    "sumTable['recoveredSum'] = sumTable.fundsReturned.cumsum()\n",
    "\n",
    "sumPlot = sumTable.plot(y=['recoveredSum','lostSum'])\n",
    "sumPlot.figure.set_size_inches(12,4)\n",
    "annot(sumPlot,'yticks', usd=True)\n",
    "sumPlot.set_title('Cumulative sum of lost and recovered funds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A nice ressemblance to the original. <br>\n",
    "### Now we break these losses into the issues that caused them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TIME SERIES\n",
    "\n",
    "def timeSeries(df, value, categories, exp=False, usd=False, freq='Y'):\n",
    "    \n",
    "    timeFormat = '%Y'\n",
    "    if freq == 'M':\n",
    "        timeFormat = '%b '+timeFormat\n",
    "\n",
    "    df['formatedDate'] = df.date.dt.strftime(timeFormat)\n",
    "\n",
    "    table = pd.pivot_table(df, values=value, index='formatedDate', columns=categories, aggfunc=np.sum)\n",
    "\n",
    "    dateRange = pd.period_range(start=min(df.date), end=max(df.date), freq=freq)\n",
    "    table = table.reindex(dateRange.strftime(timeFormat), fill_value=0)\n",
    "\n",
    "    plot = table.plot(kind='bar')\n",
    "    \n",
    "    if exp:\n",
    "        plot.set_yscale('log')\n",
    "    \n",
    "    plot.figure.set_size_inches(12,4)\n",
    "    annot(plot, 'yticks', usd)\n",
    "    \n",
    "    return plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Break down into loss by issue\n",
    "issuePlot = timeSeries(df=df.copy(), value='fundsLost', categories='issueType', exp=True, usd=True)\n",
    "issuePlot.set_title('Funds lost over time by issue, exponential scale')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using an exponential scale to make the data above more comprehensible visually. <br>\n",
    "### We note that before 2020 there were only rugpull, access control and unlabelled types of issues. Since then many different types have appeared, but none surpase the former in terms of total funds lost. <br>\n",
    "### Finally to explain the peak of lost funds in 2022 to unlabelled issues 'Other', we consider the 10 biggest losses below and see a major contribution from the Terra Classic project of 40B USD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topLoss = df.copy()\n",
    "topLoss['lostFunds'] = topLoss.fundsLost.apply(lambda x: postfix(x, usd=True))\n",
    "topLoss = topLoss.sort_values(by='fundsLost',ascending=False).head(10)\n",
    "topLoss[['projectName', 'date', 'lostFunds','category','issueType']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Break down into loss by issue\n",
    "issueDf = df[df['date']>datetime.strptime('2020-01-01','%Y-%m-%d')]\n",
    "issueCount = timeSeries(df=issueDf, value='count', categories='issueType', exp=False, usd=False, freq='M')\n",
    "issueCount.set_title('Number of breaches over time by issue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Above we see the number of attacks instead of the amount of funds lost. We shorten the range to Aug 2020 (there were very little before) and break-down to monthly data. We note a surprising jump in rugpull and honeypot attacks in late 2021 but no other obvious patterns of seasonality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we consider some of the possible relations between the different breaches. We look at heatmaps between the category of the project breached and the issue that caused it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HEATMAP OF AGGREGATED VALUES BY TWO CATEGORICAL VARIABLES\n",
    "\n",
    "def Heatmap(df, columns, index, value, usd=False, agg=np.sum):\n",
    "    table = pd.pivot_table(df, values=value, index=index, columns=columns, aggfunc=agg)\n",
    "    table = orderPivot(table)\n",
    "\n",
    "    heatmap = sns.heatmap(table, cmap='Reds', annot=True, cbar=False, yticklabels=True)\n",
    "    heatmap.figure.set_size_inches(8,12)\n",
    "    \n",
    "    annot(heatmap, 'labels', usd)\n",
    "\n",
    "    return heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HEATMAP OF TOTAL FUNDS LOST BY CATEGORY AND ISSUE TYPE\n",
    "heatmap = Heatmap(df=df, columns='issueType', index='category', value='fundsLost', usd=True)\n",
    "heatmap.set_title('Sum of lost funds by category and issue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Above we see that by far the largest combination is stablecoins with an unlabelled type of attack. Since such a large amount of funds are lost to these unlabelled 'Other' issues, it would be interesting to try to categorise them. An interesting thing to look at (for which we would need another source of data), would be the funds lost for each of these project categories as a proportion of the estimated funds currently in such projects, this could give an indication of the level of risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#consider number of breaches instead of sum of lost funds\n",
    "heatmap = Heatmap(df=df.copy(), columns='issueType', index='category', value='count', usd=False)\n",
    "heatmap.set_title('Number of breaches by category and issue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking now at the number of breaches we find a perhaps surprising result, that token projects with the honeypot issue greatly outnumber all other combinations. Especially surprising since these only lost about 7M USD in total, far less than many others. <br>\n",
    "### To look further into this we consider the average funds lost by each attack in the next heatmap, and we see that token/rugpull attacks lose on average a meager 4200 USD, compared to the billions lost in stablecoins or borrowing and lending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average funds lost by attack\n",
    "heatmap = Heatmap(df=df.copy(), columns='issueType', index='category', value='fundsLost', usd=True, agg=np.average)\n",
    "heatmap.set_title('Average funds lost by category and issue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally we consider a breakdown of the funds that were recovered. Again this shows that further categorisation could be useful since the large majority of returned funds are unlabelled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RECOVERED FUNDS BY CATEGORY AND ISSUE TYPE\n",
    "df3 = df.loc[(df.fundsReturned!=0)]\n",
    "heatmap = Heatmap(df=df3, columns='issueType', index='category', value='fundsReturned', usd=True)\n",
    "heatmap.figure.set_size_inches(5,5)\n",
    "heatmap.set_title('Sum of recovered funds by category and issue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By considering the average proportion of the funds that are recovered we see that most often, when funds are recovered, a large majority of the total funds are indeed recovered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RECOVERED FUNDS BY CATEGORY AND ISSUE TYPE\n",
    "fundsPropDf = df.loc[(df.fundsReturned!=0)]\n",
    "fundsPropDf['returnedProp'] = pd.to_numeric(fundsPropDf.fundsReturned,downcast='float')/pd.to_numeric(fundsPropDf.fundsLost,downcast='float')\n",
    "heatmap = Heatmap(df=fundsPropDf, columns='issueType', index='category', value='returnedProp', usd=True, agg=np.average)\n",
    "heatmap.figure.set_size_inches(5,5)\n",
    "heatmap.set_title('Average proportion of funds recovered (per $1) by category and issue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaway: <br>\n",
    "#### 1. There was a clear change in 2020 when many new breach types started occuring, and 2023 already looks set to match previous years. <br>\n",
    "#### 2. A deeper classification of issues and categories could be interesting, for example the Terra Classic 40B USD breach could be classified as 'failed migration' or 'panic event'. Allowing classification of a breach to multiple issues could also be of value. <br>\n",
    "#### 3. Breaking down the lost and recovered funds by categories shows some interesting patterns, and could definitely be helped by comparing these to the current state of live projects, if such data is available."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "e156622c090ea153a91a4c3841b25fcf686417a8aff31f432912ebeaeab5b9dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
